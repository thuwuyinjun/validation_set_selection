{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os,sys\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from argparse import Namespace\n",
    "from src.models.resnet3 import resnet18\n",
    "import numpy as np\n",
    "from src.exp_datasets.dataloader import dataset_wrapper, randomly_produce_valid_set\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:start\n"
     ]
    }
   ],
   "source": [
    "args = {\"data_dir\": \"data/\", \"valid_count\": 100, \"meta_lr\": 30,  \"save_path\": \"output/\",\"cuda\":True, \"lr\":0.1, \"batch_size\":128, \"test_batch_size\":128, \"epochs\":100, \"do_train\":True, \"use_pretrained_model\":True, \"lr_decay\":True, \"metric\": \"accuracy\"}\n",
    "args = Namespace(**args)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "args.logger = logging\n",
    "\n",
    "args.logger.info(\"start\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download real labeling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-02-05 00:18:34--  http://www.yliuu.com/web-cifarN/files/CIFAR-N.zip\n",
      "Resolving www.yliuu.com (www.yliuu.com)... 2607:f8b0:4004:c08::80, 172.253.63.128\n",
      "Connecting to www.yliuu.com (www.yliuu.com)|2607:f8b0:4004:c08::80|:80... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data//CIFAR-N.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 438874 (429K) [application/zip]\n",
      "Saving to: ‘data/CIFAR-N.zip.15’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 11% 1.28M 0s\n",
      "    50K .......... .......... .......... .......... .......... 23% 4.98M 0s\n",
      "   100K .......... .......... .......... .......... .......... 34% 9.00M 0s\n",
      "   150K .......... .......... .......... .......... .......... 46% 8.37M 0s\n",
      "   200K .......... .......... .......... .......... .......... 58% 7.92M 0s\n",
      "   250K .......... .......... .......... .......... .......... 69% 8.03M 0s\n",
      "   300K .......... .......... .......... .......... .......... 81% 8.03M 0s\n",
      "   350K .......... .......... .......... .......... .......... 93% 8.14M 0s\n",
      "   400K .......... .......... ........                        100% 6.29M=0.09s\n",
      "\n",
      "2023-02-05 00:18:45 (4.76 MB/s) - ‘data/CIFAR-N.zip.15’ saved [438874/438874]\n",
      "\n",
      "caution: filename not matched:  -o\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2816"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"wget \\\"http://www.yliuu.com/web-cifarN/files/CIFAR-N.zip\\\" -P \" + args.data_dir)\n",
    "# os.rename(\"CIFAR-N.zip\", os.path.join(args.data_dir, \"CIFAR-N.zip\"))\n",
    "os.system(\"unzip \" + args.data_dir + \"/CIFAR-N.zip -o -d\" + args.data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "valid_count = args.valid_count\n",
    "\n",
    "transform_train_list = [\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "]\n",
    "transform_train = transforms.Compose(transform_train_list)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=os.path.join(args.data_dir, 'CIFAR-10'),\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train,\n",
    ")\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=os.path.join(args.data_dir, 'CIFAR-10'),\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test,\n",
    ")\n",
    "\n",
    "# create a wrapper of the dataset to return the indices of each sample during training\n",
    "\n",
    "trainset = dataset_wrapper(np.copy(trainset.data), np.copy(trainset.targets), transform_train)\n",
    "testset = dataset_wrapper(np.copy(testset.data), np.copy(testset.targets), transform_test)\n",
    "validset, testset = randomly_produce_valid_set(testset, rate = 0.4)\n",
    "\n",
    "origin_labels = np.copy(trainset.targets)\n",
    "\n",
    "trainset.targets = torch.load(os.path.join(args.data_dir, \"CIFAR-N/CIFAR-10_human.pt\"))['worse_label']\n",
    "\n",
    "net = resnet18(num_classes=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrain a model without reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:start training\n"
     ]
    }
   ],
   "source": [
    "from src.main.main_train import basic_train\n",
    "trainloader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "validloader = DataLoader(validset, batch_size=args.test_batch_size, shuffle=False, num_workers=2, pin_memory=False)\n",
    "testloader = DataLoader(testset, batch_size=args.test_batch_size, shuffle=False, num_workers=2, pin_memory=False)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=args.lr,\n",
    "                momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "optimizer.param_groups[0]['initial_lr'] = args.lr\n",
    "\n",
    "mile_stones_epochs = [100, 110]\n",
    "gamma = 0.2\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=mile_stones_epochs,\n",
    "    last_epoch=-1,\n",
    "    gamma=gamma,\n",
    ")\n",
    "if args.cuda:\n",
    "    net = net.cuda()\n",
    "args.logger.info(\"start training\")\n",
    "# basic_train(\n",
    "#             trainloader,\n",
    "#             validloader,\n",
    "#             testloader,\n",
    "#             criterion,\n",
    "#             args,\n",
    "#             net,\n",
    "#             optimizer,\n",
    "#             scheduler=scheduler,\n",
    "#         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selecting meta samples with RBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==> Loading cached model...\n",
      "INFO:root:==> Loading cached model successfully\n",
      "391it [00:17, 22.33it/s]\n",
      "INFO:root:max norm of the representation:5.245381\n",
      "INFO:root:min norm of the representation:0.402533\n",
      "INFO:root:extra representation starting from epoch 89\n",
      "INFO:root:==> Loading cached model at epoch 89\n",
      "INFO:root:==> Loading cached model successfully\n",
      "391it [00:20, 18.94it/s]\n",
      "INFO:root:max norm of the representation:5.058860\n",
      "INFO:root:min norm of the representation:0.475458\n",
      "INFO:root:==> Loading cached model at epoch 109\n",
      "WARNING:root:Could not find cached model: output/model_109\n",
      "INFO:root:==> Loading cached model at epoch 129\n",
      "WARNING:root:Could not find cached model: output/model_129\n",
      "INFO:root:==> Loading cached model at epoch 149\n",
      "WARNING:root:Could not find cached model: output/model_149\n",
      "INFO:root:==> Loading cached model at epoch 169\n",
      "WARNING:root:Could not find cached model: output/model_169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [01:34<00:00,  1.05it/s]\n",
      "[running kmeans]: 152it [03:34,  1.41s/it, center_shift=0.000039, iteration=152, tol=0.000100]\n",
      "INFO:root:cluster count before and after:(100,92)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max dist sample to assigned cluster mean:: 0.5743220448493958\n",
      "min dist sample to other cluster mean:: 0.19798117876052856\n",
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [01:20<00:00,  1.13it/s]\n",
      "[running kmeans]: 119it [02:37,  1.32s/it, center_shift=0.000054, iteration=119, tol=0.000100]\n",
      "INFO:root:cluster count before and after:(92,92)\n",
      "INFO:root:unique cluster count::92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max dist sample to assigned cluster mean:: 0.5697579979896545\n",
      "min dist sample to other cluster mean:: 0.19198131561279297\n"
     ]
    }
   ],
   "source": [
    "from src.main.main_train import load_checkpoint2\n",
    "from src.main.find_valid_set import get_representative_valid_ids_rbc, get_representative_valid_ids_gbc\n",
    "\n",
    "#load the pretrained models for selecting meta samples\n",
    "args.prev_save_path = args.save_path\n",
    "\n",
    "net = load_checkpoint2(args, net)\n",
    "\n",
    "#label_aware is false for noisy labels while label_aware is true for imbalanced dataset\n",
    "args.label_aware=False\n",
    "args.all_layer=False\n",
    "args.model_prov_period=20\n",
    "args.do_train=False\n",
    "args.bias_classes=False\n",
    "args.weight_by_norm=True\n",
    "\n",
    "valid_ids, new_valid_representations = get_representative_valid_ids_rbc(criterion, optimizer, trainloader, args, net, valid_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7edfc12edf23d04e14133e61de4fea84e7a1127629723b8a14798f7b33b14da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
